---
title: "Propagating Uncertainty in a Mixed Stock Analysis Example"
output: html_document
runtime: shiny
---
We propose a simple model for looking at what it really means to do probabilistic
inference and "propagate uncertainty" by using a simple genetic stock ID model.

Our goal will be to estimate $m$ the fraction of fish from population 1 (as opposed to
population 2) using reads obtained from $N$ individuals (sampled from a mixture of
individuals from the two populations) at $L$ loci.  We will assume
that the allele frequencies in the two populations, vector $p_1$ and $p_2$, respectively,
are known without error.  But that we do not know, ahead of time, which population each
individual comes from. 

The SNP allele frequencies in population 1, $p_1$, are drawn from a Beta(1.5,1.5) distribution,
and then the frequencies in population 2, $p_2$, are drawn using the Nicholson model with $F_\mathrm{ST} = f$.
This model is the same as used for the correlated allele frequencies model in the program
`structure`.  Basically we have:
$$
p_2 \sim \mathrm{Beta}\biggl( 
\frac{1-f}{f}p_1,
\frac{1-f}{f}(1 - p_1),
\biggr)
$$
Below you will be able to enter values for the parameters:

- $m$: the fraction of individuals in the mixture from population 1.
- $N$: the size of the sample from the mixture.
- $L$: the number of SNP loci available.
- $f$: the degree of differentiation (like $F_\mathrm{ST}$) you wish to have between the populations
- $R$: the average read depth per site from an individual.

When you are starting out, you might as well use the default values, and then you can
start playing around with things further.
When values you want are in the input fields, you proceed by hitting the
big orange, **Run The Simulation** button.   First, this will simulate allele frequencies for the two
populations. These are presented in a scatter plot.  Subsequently the genotypes (as well as reads) from a
sample of individuals from the
mixture are simulated, and then the program proceeds to calculate the posterior distribution for
the mixing proportion, $m$, as well as the
posterior probabilities of group membership for each individual, and it displays these values
in a few plots.  

The posterior distribution of the mixing proportion and the posterior probabilities of group
membership are computed using three different methods, as described in the numbered
list below. In all cases, it is assumed
that the allele frequencies in each population are known without error. (This is not realistic,
but it simplifies the calculation and allows the inference procedure to occur quickly
enough to wrap it all in a Shiny App).

1. `from_the_actual_genotypes`: For these estimates, the program uses the _true genotypes_ of the individuals.
This is as good as you could possibly do---you effectively know the genotypes of all individuals with no error
or uncertainty. So, this is close to "the truth," but you typically will not achieve this with
low coverage sequencing.

2. `from_genotype_likelihoods`: In this case each individual has a randomly distributed number of reads at
each locus. These are independently Poisson-distributed numbers with an average value of $\bar{R}$ (read this
as "R-bar"), and
we assume no sequencing error. From these
reads, the genotype likelihood is calculated, and then _the likelihoods themselves_ are used in downstream
analyses.  This is what is often called  "_propagating the uncertainty_" to the downstream analyses.  Thus, rather
than having to call genotypes (for example, "It's an AA at this locus!") we use the likelihoods to express
the degree of information we have that the true genotype is one of the three possible values (for example,
"The likelihood is 0.666 that it is an AA, 0.333 that it is and AC, and 0.0 that it is a CC).  (Note, one of the
goals of this notebook is to demonstrate that with low
read depths, this is the way things should be done---propagating the uncertainty and imbedding it
all in a probability model for doing the inference!).

3. `from_max_likelihood_assigned_genotypes`: In this case, we have been forced to call genotypes using the
genotype likelihoods.  And we have done so by choosing the genotype with the highest likelihood. (Since we
don't know which population each individual comes from, we can't use the genotype posterior, because we don't
know the allele frequency to use as a prior).  This is what one typically does _not_ want to do, but many programs
(like GATK) will, by default, return this type of maximum-liklihood genotype call, even if you only have one
read at the site in an individual!

The default values are set for a case where the `from_max_likelihood_assigned_genotypes` does particularly
poorly: the posterior distribution of $m$ (colored in green) is not very wide (so you think you have an accurate, good estimate), but
typically does not overlap the true value!

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(cowplot)
source("R/mixing-prop-estimation-funcs.R")
```

```{r, echo=FALSE, fig.height = 20}
inputPanel(
  numericInput(
    "m", 
    label = "m: mixing proportion",
    min = 0.00, 
    max = 1.0, 
    value = 0.9,
    step = 0.01
  ),
  numericInput(
    "N", 
    label = "N: number of individuals",
    min = 50, 
    max = 5000, 
    value = 500,
    step = 50
  ),
  numericInput(
    "L", 
    label = "L: number of SNPs",
    min = 10, 
    max = 5000, 
    value = 100,
    step = 10
  ),
  numericInput(
    "f", 
    label = "f: degree of differentiation",
    min = 0.00, 
    max = 1.0, 
    value = 0.05,
    step = 0.01
  ),
  numericInput(
    "R", 
    label = "R: average read depth",
    min = 0.00, 
    max = 1.0, 
    value = 0.1,
    step = 0.01
  ),
  actionButton(
    "run_sim", 
    "Run the Simulation",
    class = "btn-warning"
  )
)



# simulate new dataset of genotypes
plots <- eventReactive(
  input$run_sim,
  {
    simulate_data_and_inferences(
      N = input$N,
      L = input$L, 
      f = input$f, 
      mix = input$m,
      AveRD = input$R
    )
  }
)


renderPlot(
  {
    
    # get a summary of the read depths if they are available
    PL <- plots()
    
    cowplot::plot_grid(
      PL$afreq_plot,
      PL$mixing_proportion_posteriors,
      PL$ass_prob_histos,
      ncol = 1
    )
  },
  height = 900
)

```


### Here is what the DAG for this model looks like:

```{r, echo=FALSE, out.width='70%'}
knitr::include_graphics("diagrams/dag_for_hands_on_003.svg")
``` 

* The allele frequencies at each locus $\ell = 1,\ldots,L$, in population 1 ($p_{1,\ell}$) and
in population 2 ($p_{2,\ell}$) are assumed known
* The read depths of different alleles $\boldsymbol{R}_{i,\ell}$ in each individual $i$ and each locus $\ell$ are observed.
* The mixing proportion $m$, the individual population origin, $Z_i$, or each individual $i$, and the the allelic
type of the two gene copies at each locus $\ell$ in each individual $i$, ($Y_{1,i,\ell}$ and $Y_{2,i,\ell}$
are all unknown, and can be inferred.


### Some things to do/think about

1. Is there a problem when read depths are quite high?
2. What happens if $m$ is near 0.5?
3. What happens when $f$ gets even smaller?
4. Can you find a combination of parameters that shows problematic
behavior of the `from_max_likelihood_assigned_genotypes`, even when
read depths are as high as 1X?
